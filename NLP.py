# -*- coding: utf-8 -*-
"""notebookbddc093bc6 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kH2HiRKYbpWhQbxurTQ_BOu0byw4gvNl
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'int-sys-main:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4425003%2F7601220%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240211%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240211T061056Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da887f9170e4d6d3e99d19b18608a13acca66e3da5b0c4964959bd80183a0a9f4dca6adf0e64662039a5ee2bf48456ff7f47b7106ab8efb1b6af361cd9775277e82329a88712f4c0f1e884c92155d07675f889142668cc4ff4235bc9c176a5cf86c78787293a5dcc3560ddaf2fcb1c2be2ea993f4c455bbf8eaba5bb7782662d9b706ad72df36effc6225c4ac95396ba9ec87aef9f960b47b6188e13c9b06306bf7f1f89c157f986a3cc994ca46ef77dbfd038fc96be3e934c5af1d77ac990584d17927190636000cd5bc43ea96473b43990562919bde53bee2bf305112d7b0c4ef3d294d7be900210842b8da63fd59c654761fe65036d17b0e9f6d621a7acf7f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np
import pandas as pd
import re

sntncs=[]
with open('/kaggle/input/int-sys-main/imdb_labelled.txt', 'r') as file:
    content = file.read()
    label = [int(s) for s in re.findall(r'\t\d+\b', content)]
    label.append(0)
    content = re.sub(r'[^a-zA-Z\s]', '', content)

#     content = content.replace('0','1')
    sall = content.split('\t\n')

    for i in sall:
        sntncs.append(i.strip())

for i in range(len(sntncs)):
    sntncs[i] = sntncs[i].lower()

"""*italicised text*"""

df_word = pd.DataFrame(columns=['sentence'])
for i in range(len(sntncs)):
    df_word.loc[len(df_word)] = sntncs[i]

# df_word['label'] = label
df_word.head()

word_to_vec_map = {}
with open('/kaggle/input/int-sys-main/glove.6B.50d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        word_to_vec_map[word] = coefs

# Map words in the dataset to their corresponding word vectors
def words_to_vectors(words):
    return [word_to_vec_map[word] for word in words if word in word_to_vec_map]

imdb_data = pd.DataFrame(columns=['sentence'])
imdb_data['sentence'] = df_word['sentence'].apply(words_to_vectors)

imdb_data['label'] = label
imdb_data

nan=[]
for i in range(len(imdb_data['sentence'])):
    if imdb_data['sentence'][i]==[]:
        nan.append(i)

nan

imdb_data.drop(nan, inplace=True)
imdb_data.reset_index(inplace=True)
imdb_data.drop('index',axis=1, inplace=True)
imdb_data

# max_len = max(imdb_data['sentence'].apply(len))
# for i in range(len(imdb_data['sentence'])):
#     ll = len(imdb_data['sentence'][i])
#     for j in range(max_len-ll):
#         imdb_data['sentence'][i].append(np.zeros((50,)))

from keras.preprocessing.sequence import pad_sequences

max_len = max(imdb_data.apply(len))
padded_sequences = pad_sequences(imdb_data['sentence'], maxlen=max_len, dtype='float32')
padded_sequences

from sklearn.model_selection import train_test_split

sentences_train, sentences_test, y_train, y_test = train_test_split(padded_sequences, imdb_data['label'], test_size=0.25, random_state=1000)

from keras import models, layers

model = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# Train the model with padded_sequences as input and labels as output
history = model.fit(sentences_train, y_train, epochs=200, verbose=False, batch_size=32, validation_data=(sentences_test, y_test))

loss, accuracy = model.evaluate(sentences_test, y_test)
print("Testing Accuracy:  {:.4f}".format(accuracy))

from keras.optimizers import Adam, SGD

# Define a list of learning rates to experiment with
learning_rates = [0.001, 0.0001, 0.00001]
i=1

for lr in learning_rates:
    model1 = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
    ])
        # Choose an optimizer with the current learning rate
    optimizer1 = Adam(lr=lr)

    # Train the model with the current learning rate
    model1.compile(optimizer=optimizer1, loss='binary_crossentropy', metrics=['accuracy'])
    print(model1.summary(),"\n...........")

    history1 = model1.fit(sentences_train, y_train, epochs=200, batch_size=32)

    loss, accuracy = model1.evaluate(sentences_test, y_test)
    print(f"{i}: "+"Testing Accuracy of :  {:.4f}".format(accuracy))
    i+=1

# Choose two different optimizers
optimizer1 = Adam(lr=0.0001)

model2 = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
    ])
# Train the model with the first optimizer
model2.compile(optimizer=optimizer1, loss='binary_crossentropy', metrics=['accuracy'])
model2.summary()

history2 = model2.fit(sentences_train, y_train, epochs=100, batch_size=32)

loss, accuracy = model2.evaluate(sentences_test, y_test)
print("Testing Accuracy:  {:.4f}".format(accuracy))

optimizer2 = SGD(lr=0.00001, momentum=0.9)
model2 = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
    ])

# Train the model with the second optimizer
model2.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])

history3 = model2.fit(sentences_train, y_train, epochs=100, batch_size=32)
model2.summary()
loss, accuracy = model2.evaluate(sentences_test, y_test)
print("Testing Accuracy:  {:.4f}".format(accuracy))

optimizer3 = SGD(lr=0.00001, momentum=0.9)
model3 = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(128, activation='linear'),
    layers.Dense(64, activation='sigmoid'),
    layers.Dense(32, activation='sigmoid'),
    layers.Dense(1, activation='relu')
    ])
model3.compile(optimizer=optimizer3, loss='binary_crossentropy', metrics=['accuracy'])

model3.summary()

history3 = model3.fit(sentences_train, y_train, epochs=100, batch_size=32)

loss, accuracy = model3.evaluate(sentences_test, y_test)
print("Testing Accuracy:  {:.4f}".format(accuracy))

optimizer4 = SGD(lr=0.00001, momentum=0.9)
model4 = models.Sequential([
    layers.Dense(max_len, input_shape=(max_len, 50)),
    layers.Dense(128, activation='linear'),
    layers.Dense(64, activation='linear'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='relu')
    ])
model4.compile(optimizer=optimizer4, loss='binary_crossentropy', metrics=['accuracy'])

model4.summary()

history4 = model4.fit(sentences_train, y_train, epochs=100, batch_size=32)

loss, accuracy = model4.evaluate(sentences_test, y_test)
print("Testing Accuracy:  {:.4f}".format(accuracy))

import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    # val_acc = history.history['val_acc']
    loss = history.history['loss']
    # val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    # plt.plot(x, val_acc, 'r', label='Validation acc')
    # plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    # plt.plot(x, val_loss, 'r', label='Validation loss')
    # plt.title('Training and validation loss')
    plt.legend()

plot_history(history1)

plot_history(history)

plot_history(history2)

plot_history(history3)

plot_history(history4)

https://realpython.com/python-keras-text-classification/